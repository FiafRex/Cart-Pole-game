{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKHos3umDykn/YqZ4JgJhK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FiafRex/Cart-Pole-game/blob/main/Mainfile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Dependencies"
      ],
      "metadata": {
        "id": "Lrx1ZUpKmLRp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW6R7vJhmIqj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import gym #by openAI to use with reinfornced DL\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines import necessary libraries and modules for the code to run. These include libraries for handling randomness, interacting with the OpenAI Gym environment, working with arrays, using deque for experience replay, and building and training neural networks with Keras."
      ],
      "metadata": {
        "id": "4QXkCQ8ExZqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the environment"
      ],
      "metadata": {
        "id": "Dk9m9WuFmg_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P43u-sQ8xc0G",
        "outputId": "7b9413fc-e353-435c-f6f6-43f32d2e2abf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line creates an environment called 'CartPole-v1' using the Gym library. This environment represents a cart and pole balancing problem, where the goal is to balance a pole on a moving cart for as long as possible."
      ],
      "metadata": {
        "id": "v7A1kVcExdOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 1.0\n",
        "epsilon_decay = 0.995\n",
        "epsilon_min = 0.01"
      ],
      "metadata": {
        "id": "qFjFS0ExxheK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "parameters related to exploration. Epsilon is the exploration rate, starting at 1.0 and decaying over time (at 0.995 rate)."
      ],
      "metadata": {
        "id": "RlJq2Zq4xjja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_action(model, state):\n",
        "    if np.random.rand() <= epsilon:\n",
        "        return random.randrange(number_of_actions)\n",
        "    else:\n",
        "        action_values = model.predict(state)\n",
        "        return np.argmax(action_values[0])\n"
      ],
      "metadata": {
        "id": "Wd4UDaTix7-t"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implements agent's policy for selection actions. Taking the current state and the model (neural network) as input. With epsilon, it chooses a random action, otherwise selecting teh action with the highest predicted Q-value (expected reward) using the model"
      ],
      "metadata": {
        "id": "vm7jRzKkx8zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.95\n"
      ],
      "metadata": {
        "id": "N_H43EtUyMvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines discount facotr (gamma)-which deterimines the importance of future rewards in teh agent's decision making process. A value of 0.95-means that future rewards are discounted by 5% at each step."
      ],
      "metadata": {
        "id": "XrqyHZwfyNEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, memory, epsilon):\n",
        "    batch = random.sample(memory, batch_size)\n",
        "    for state, action, reward, next_state, done in batch:\n",
        "        target = reward\n",
        "        if not done:\n",
        "            target = reward + gamma * np.amax(model.predict(next_state)[0])\n",
        "        state_target = model.predict(state)\n",
        "        state_target[0][action] = target\n",
        "        model.fit(state, state_target, epochs=1, verbose=0)\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "    return model, epsilon\n"
      ],
      "metadata": {
        "id": "QDz5IC6SyZdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implements the experience replay mechanism and updates model based on past expereiences. It samples a batch of expereinces from memory-calculating the target Q value for each experience, updates teh Q value of the chosen action in the current state, and trains the model using the current state and updated target values. It also decays epsilon, when and if its above the minimum value."
      ],
      "metadata": {
        "id": "Fd5SCWGwyaFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_size = env.observation_space.shape[0]\n",
        "number_of_actions = env.action_space.n\n"
      ],
      "metadata": {
        "id": "609Pk2NeyzZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines extract the state size and number of actions from the environment. The state size-represents the number of observations in the environment and the number of actions represents the number of possible actions, the agent can take."
      ],
      "metadata": {
        "id": "z8SX-6zByzxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
        "model.add(Dense(24, activation='relu'))\n",
        "model.add(Dense(number_of_actions, activation='linear'))\n"
      ],
      "metadata": {
        "id": "7UkL9NDBzAiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code defines a neural network model using Kears, a simple feedforward network with two hidden layyers, containing 24 neurons and an output layer with anumber of neurons = number of ations. The activation functions used are ReLu for the hidden layer and linear for Output layer/"
      ],
      "metadata": {
        "id": "BUK5raa7zDM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse', optimizer='adam')\n"
      ],
      "metadata": {
        "id": "mZFtuAqNzRwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line compiles the model, specifies the loss function and optimizer"
      ],
      "metadata": {
        "id": "iot6nTMlzSFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = deque(maxlen=2000)\n",
        "number_of_episodes = 1000\n",
        "batch_size = 32\n"
      ],
      "metadata": {
        "id": "8lYEwgdMzYM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize a memroy buffer to store past exp, set the number of training episodes and specify the batch size for experience replay."
      ],
      "metadata": {
        "id": "6-rCK2WPzYlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(number_of_episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    for time_step in range(500):\n",
        "        env.render()\n",
        "        action = play_action(model, state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        if done:\n",
        "            reward = -15\n",
        "        else:\n",
        "            reward = reward\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "        if done:\n",
        "            print(\"Number of points:\", time_step)\n",
        "            break\n",
        "        if len(memory) > batch_size:\n",
        "            model, epsilon = fit(model, memory, epsilon)\n"
      ],
      "metadata": {
        "id": "8K5ZdZ2SzlZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main training loop for agenmt. It iterates over a specified number of eps, where each ep represents a complete game. In each ep, the agent interacts with the environment, selects actions based on policy-observes consequences, updates memroy with experiences and trains model using experience replay."
      ],
      "metadata": {
        "id": "KbCnV_odzpPV"
      }
    }
  ]
}